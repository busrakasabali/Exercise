{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSX0SsMuPUSh"
      },
      "source": [
        "# Tutorial 5 - Word Embeddings\n",
        "\n",
        "In the lectures, so far you have covered NLP processing of textual data and word embeddings. Therefore, in today's notebook we will revisit the algorithm Word2Vec (W2V) that you can use to learn word embeddings with two models, i.e., CBOW and Skip-Gram. While the two flavours of W2V differ in how they are modeling token embeddings, both models aim at producing dense numerical vector representations, that capture the semantic relationships in the input textual samples. Once we have covered how to create pretrained embedding dictionaries compatible with keras, we will generate two-dimensional representation of the trained word embeddings that is suitable for plotting purposes. Afterward, we will use the embeddings from W2V in an MLP-based network trained for sentiment classification. <br>\n",
        "\n",
        "Here is the outline of today's notebook:\n",
        "*   Word2Vec: Implementation of an Embedding Layer Dictionary with CBOW and Skip-Gram (Demo).\n",
        "*   Plotting Embeddings using t-SNE (Exercise 1).\n",
        "*   MLP-based Neural Network with W2V Embeddings for Sentiment Classification (Exercise 2).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nEgzbuRiPUSq"
      },
      "outputs": [],
      "source": [
        "# required packages:\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from sklearn.metrics import recall_score,precision_score,roc_auc_score\n",
        "import numpy as np\n",
        "from keras.layers import TextVectorization\n",
        "from gensim.models import Word2Vec\n",
        "import time\n",
        "import keras\n",
        "from keras import Sequential\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USroVvo7PUSt"
      },
      "source": [
        "## **1. Word2Vec: Implementation of an Embedding Layer Dictionary with CBOW and Skip-Gram** (Demo)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X90G5h2PUSu"
      },
      "source": [
        "We use W2V to train word embeddings, which capture the contextual meaning of individual tokens:<br>\n",
        "<img src=\"https://github.com/Humboldt-WI/demopy/raw/main/W2V.PNG\" width=\"1140\" height=\"610\" alt=\"W2V\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pW6knygPUSv"
      },
      "source": [
        "In W2V, each word is represented by two vectors of dimension *ùëë*, as words take both roles, i.e., context and target word. The word vectors are the parameters of the neural network, which we train on our corpus of textual data for language modeling purposes. The goal is to learn low-dimensional, dense representation of words as numerical continuous vectors, which enable ML models to understand the meaning and the semantics of words algorithmically. Therefore, language modeling can be regarded as the upstream task, whereas, e.g., sentiment classification using the pretrained embeddings from W2V would be the downstream task. There are two variants of W2V, which we can use to obtain word embeddings: <br>\n",
        "\n",
        "<img src=\"https://github.com/Humboldt-WI/demopy/raw/main/CBOW_and_SkipGram.PNG\" width=\"1450\" height=\"390\" alt=\"CBOW_and_SkipGram\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q93kFxM8PUSx"
      },
      "source": [
        "The Continuous Bad of Word (CBOW) model differs from the Skip-Gram model mainly in that the former predicts the center word from surrounding context tokens, whereas the latter predicts the context from the center word. The surrounding words are quantified by defining a context window. In the above visualization, the context window is set to 2 tokens. The training process involves parsing the textual samples with context size 2, and sliding the training tuples consisting of inputs and targets until the end of the sentences. Language modeling does not necessitate target labels or text annotation, as the training inputs and outputs are obtained from parsing the textual samples. Thus, we call such training process self-supervised. W2V in its two flavours is trained with a shallow neural network. For simplicity purposes, assume you have a single token as the input and the following word as the target:<br>\n",
        "<img src=\"https://github.com/Humboldt-WI/demopy/raw/main/W2V_Shallow_Network_Example.PNG\" width=\"1450\" height=\"410\" alt=\"W2V_Shallow_Network_Example\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h17mVmw3PUSz"
      },
      "source": [
        "The input layer of the network represents a one-hot-vector, which has a positive value at the index of the input word. The dot product of the one-hot-vector with the continuous trainable weights of the shallow network amounts to indexing the vector of weights in the hidden layer associated with the input word *can*. This is also the vector of weights that we will use as our word embeddings in the downstream task. If we choose to train our own embeddings using W2V instead of downloading pretrained embeddings, then, first, we would have to clean our dataset. Thus, let's import the IMDB dataset, and clean it using our `NLP_preprocessing_pipeline` function from the previous tutorial notebook. While previously the function returned a list of cleaned tokens, in this notebook we will design our function to return the whole cleaned string of the textual samples, as we will later feed these cleaned strings to a `TextVectorization` layer from `keras`:<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE3ZF49GPUS1",
        "outputId": "2ffb3f58-f7c4-4089-b6c0-1a2daac55568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# Lemmatize with POS Tag (Parts of Speech tagging)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def get_wordnet_pos(word:str)->str:\n",
        "    \"\"\"Map POS tag to first character for lemmatization\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pos: str\n",
        "        The positional tag of speech retrieved from wordnet database.\n",
        "    \"\"\"\n",
        "\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    pos=tag_dict.get(tag,wordnet.NOUN)\n",
        "\n",
        "    return pos\n",
        "\n",
        "def NLP_preprocessing_pipeline(textual_sample:str)->list:\n",
        "    '''\n",
        "    Implements 7 steps of an NLP preprocessing pipeline.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    textual_sample:str\n",
        "        The input text that requires preprocessing\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    preprocessed_textual_sample:str\n",
        "        The textual sample after each of the 7 preprocessing steps have been applied.\n",
        "\n",
        "    '''\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    #Removing of URLs:\n",
        "    preprocessed_textual_sample = re.sub(\"http\\S+\", \"\",textual_sample)\n",
        "\n",
        "    #Removing of HMTL tags:\n",
        "    preprocessed_textual_sample = BeautifulSoup(preprocessed_textual_sample).get_text()\n",
        "\n",
        "    #Removing of non-alphabetic characters:\n",
        "    preprocessed_textual_sample = re.sub(\"[^a-zA-Z]\", \" \",preprocessed_textual_sample)\n",
        "\n",
        "    #Changing all tokens to lower case:\n",
        "    preprocessed_textual_sample = preprocessed_textual_sample.lower()\n",
        "\n",
        "    #Tokenization:\n",
        "    preprocessed_textual_sample=nltk.word_tokenize(preprocessed_textual_sample)\n",
        "\n",
        "    #Stopwords removal:\n",
        "    preprocessed_textual_sample = [w for w in preprocessed_textual_sample if w not in stopwords.words(\"english\")]\n",
        "\n",
        "    #Lemmatization:\n",
        "    preprocessed_textual_sample=[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in preprocessed_textual_sample]\n",
        "    preprocessed_textual_sample=' '.join(preprocessed_textual_sample) #Turn a list of words into a single string, with spaces between the words.\n",
        "\n",
        "    return preprocessed_textual_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "uJ-W_uLOPUS6",
        "outputId": "658584c9-056e-4663-e036-212422fa3354"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "5  Probably my all-time favorite movie, a story o...  positive\n",
              "6  I sure would like to see a resurrection of a u...  positive\n",
              "7  This show was an amazing, fresh & innovative i...  negative\n",
              "8  Encouraged by the positive comments about this...  negative\n",
              "9  If you like original gut wrenching laughter yo...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60f2f6ba-7ab5-4afd-be29-1ca553b62a1f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60f2f6ba-7ab5-4afd-be29-1ca553b62a1f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-60f2f6ba-7ab5-4afd-be29-1ca553b62a1f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-60f2f6ba-7ab5-4afd-be29-1ca553b62a1f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-eec3424b-87db-4358-949f-a58557d2b3f7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eec3424b-87db-4358-949f-a58557d2b3f7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-eec3424b-87db-4358-949f-a58557d2b3f7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4997,\n        \"samples\": [\n          \"We brought this film as a joke for a friend, and could of been our worst joke to play. The film is barely watchable, and the acting is dire. The worst child actor ever used and Hasslehoff giving a substandard performance. The plot is disgraceful and at points we was so bored we was wondering what the hell was going on. It tries to be gruesome in places but is just laughable.<br /><br />Just terrible\",\n          \"The movie is really about choices. In the oppressed state of affairs as seen in Fire, where good women had to be obedient and do what was correct in the eyes of tradition, there seemed few options for Radha and Sita. However, granted that it was not their only option. What is life without desire, Radha questions Ashok. Yes, it's true that life provides us with a number of options but how many we can take depends on a number of external factors. When your world is confined to a small Indian household, being a dutiful daughter-in-law to a silent but observant still powerful matriarch, a dutiful wife of 13 years to a man who has taken a vow of celibacy due to your not being able to have a child, a man who only wants you lying next to him to prove his strength in eliminating his desires. I felt the ladies had little choice but to find solace in each other's company. I guess the fact that so many women applauded Ms Mehta's work, was because it provided them with an option to think for themselves. An option to do what was perhaps unacceptable. The lesbian scenes I felt was merely to put that point across. Every scene in the movie from the first at the Taj Mahal to the last at the Mosque, is etched in my mind. How frustrating to be a prisoner of your feelings and desires. To feel that you had to forgo the human touch to be a dutiful wife just because it is expected of you. To have to suppress any desire you might have and to crave for the human touch. What then is the meaning of our existence one wonders. In the scene where Sita is crying alone in the room and Radha comes in to comfort her, their lips accidentally brush against each others and it awakens a feeling in them. Something they have both been deprived of.<br /><br />Bravo to Ms Mehta in translating her vision so clearly. I especially like the flashbacks to the young Radha trying to 'see' for the ocean. It is a metaphor for freedom. Freedom to choose, freedom to transport ourselves to places we would normally be unable to reach. In those scenes, it is gently told to us that her sense of duty has also been passed down from her mother who I assume lives within the rich Indian traditions of duty in marriage. The movie is beautifully filmed and enhanced by the musical score by A.R. Rahman. Since the film, I have become ardent fans of the two lead actresses and the director. I look forward to more of Ms Deepa's future productions.\",\n          \"2005 was one of the best year for movies. We had so many wonderful movies, like Batman Begins, Sin City, Corpse Bride, A History of Violence.....Coming up we also got Brokeback Mountain, King Kong....But if this year the only great movie that came out was Everything Is Illuminated, then we wouldn't miss all this year has brought. The first movie as a director of the talented Liev Schreiber is a delightful, heart-warming, touching drama that also brings one of Elijah Wood's best roles. He is perfect as Jonathan, a curious man that heads for Ukraine to find the woman who saved his Grandfather in World War II. Liev Schreiber, who also writes the movie, conducts a masterpiece, with memorable scenes and (a lot of) funny quotes. This here is a genuine mixture of Comedy with Drama, bringing a movie that will be commented years from now. A serious Oscar contender, Everything is Illuminated is a powerful, original, and, why not say, illuminated movie. But there's one thing you should remember while entering the movie: leave normal behind. This is special.------9/10\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df = pd.read_csv(\"IMDB-50K-Movie-Review.zip\", sep=\",\", encoding=\"ISO-8859-1\").iloc[:5000,:]\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "QYbBxLXRPUTC",
        "outputId": "5d9c9f84-b854-4268-df76-60401aa225bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-1725950941.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt_tab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'averaged_perceptron_tagger_eng'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNLP_preprocessing_pipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-1299043318.py\u001b[0m in \u001b[0;36mNLP_preprocessing_pipeline\u001b[0;34m(textual_sample)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m#Lemmatization:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mpreprocessed_textual_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed_textual_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mpreprocessed_textual_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_textual_sample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Turn a list of words into a single string, with spaces between the words.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-1299043318.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m#Lemmatization:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mpreprocessed_textual_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed_textual_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mpreprocessed_textual_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_textual_sample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Turn a list of words into a single string, with spaces between the words.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-1299043318.py\u001b[0m in \u001b[0;36mget_wordnet_pos\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     tag_dict = {\"J\": wordnet.ADJ,\n\u001b[1;32m     18\u001b[0m                 \u001b[0;34m\"N\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[1;32m    168\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_conf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "X=df['review'].apply(NLP_preprocessing_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "uKRbN4yOvkcx",
        "outputId": "66de6417-c59d-4af4-a4e8-0813f72af3f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    one reviewer mention watch oz episode hooked r...\n",
              "1    wonderful little production film technique una...\n",
              "2    thought wonderful way spend time hot summer we...\n",
              "3    basically family little boy jake think zombie ...\n",
              "4    petter mattei love time money visually stun fi...\n",
              "5    probably time favorite movie story selflessnes...\n",
              "6    sure would like see resurrection date seahunt ...\n",
              "7    show amaze fresh innovative idea first air fir...\n",
              "8    encourage positive comment film look forward w...\n",
              "9    like original gut wrench laughter like movie y...\n",
              "Name: review, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one reviewer mention watch oz episode hooked r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wonderful little production film technique una...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thought wonderful way spend time hot summer we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically family little boy jake think zombie ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter mattei love time money visually stun fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>probably time favorite movie story selflessnes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>sure would like see resurrection date seahunt ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>show amaze fresh innovative idea first air fir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>encourage positive comment film look forward w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>like original gut wrench laughter like movie y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-vOgCZihPUTH"
      },
      "outputs": [],
      "source": [
        "#We will save the cleaned version of our movie reviews as the preprocessing takes a while to complete.\n",
        "with open('cleaned_X_str.pkl','wb') as f:\n",
        "    pickle.dump(X,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u1z-1RFnPUTI"
      },
      "outputs": [],
      "source": [
        "with open('cleaned_X_str.pkl','rb') as f:\n",
        "    X=pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "dbMfp7rCPUTI",
        "outputId": "7be998e7-0709-4dc4-fbb2-fe49ba32151e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    one reviewer mention watch oz episode hooked r...\n",
              "1    wonderful little production film technique una...\n",
              "2    thought wonderful way spend time hot summer we...\n",
              "3    basically family little boy jake think zombie ...\n",
              "4    petter mattei love time money visually stun fi...\n",
              "5    probably time favorite movie story selflessnes...\n",
              "6    sure would like see resurrection date seahunt ...\n",
              "7    show amaze fresh innovative idea first air fir...\n",
              "8    encourage positive comment film look forward w...\n",
              "9    like original gut wrench laughter like movie y...\n",
              "Name: review, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one reviewer mention watch oz episode hooked r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wonderful little production film technique una...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thought wonderful way spend time hot summer we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically family little boy jake think zombie ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter mattei love time money visually stun fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>probably time favorite movie story selflessnes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>sure would like see resurrection date seahunt ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>show amaze fresh innovative idea first air fir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>encourage positive comment film look forward w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>like original gut wrench laughter like movie y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "X[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51FDREnkPUTJ"
      },
      "source": [
        "Next, we will build our vocabulary with the `TextVectorization` layer. The latter transforms the input strings to a list of integer token indices, which are associated with a unique word in our vocabulary. We learn the vocabulary by calling the function `adapt`. When the layer is adapted, it learns the frequency of the individual tokens in the dataset. If we specify a maximum size of our vocabulary, e.g., 15k, then the layer would create a vocabulary containing the 15k most frequently encountered words in the cleaned textual samples. Words outside of this vocabulary get mapped to the UNK-token, i.e., out-of-vocabulary token. For consistency purposes, we will also specify the maximal sequence length for each textual sample. For instance, if we set the sequence length to a maximum of 100 tokens, then textual samples with less tokens will get padded with zeros to a length of 100. Similarly to other preprocessing techniques that we have applied so far in this semester, the layer `TextVectorization` is adapted on the train set only. For this reason, we will split our cleaned data *X* into train and test set before the learning of the vocabulary takes place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH08PxoSPUTM",
        "outputId": "58b3d5d5-b74e-486e-e575-1da13db4c696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'movie', 'film', 'one', 'make', 'like', 'see', 'get', 'well', 'time', 'good', 'watch', 'bad', 'go']\n"
          ]
        }
      ],
      "source": [
        "#Split into train and test subsets:\n",
        "y=df['sentiment'].map({'positive':1,'negative':0}).values  # map text-based class labels to numbers\n",
        "Xclean_train, Xclean_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 5)  # data partitioning\n",
        "\n",
        "# Create a vectorization layer\n",
        "vocab_size = 15000\n",
        "seq_length = 100\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize = None,\n",
        "    #since we have cleaned our data already,\n",
        "    # we pass None to the text standardization parameter.\n",
        "    max_tokens = vocab_size,\n",
        "    output_sequence_length = seq_length)\n",
        "\n",
        "#Neural networks require inputs of uniform shape (same length), especially when using batches.\n",
        "#output_sequence_lengt parameter guarantees that every vectorized sentence is a fixed-length sequence, which is critical for:\n",
        "\n",
        "#Learn the vocabulary on the train set:\n",
        "vectorize_layer.adapt(Xclean_train)\n",
        "\n",
        "#print the first 15 tokens in the vocabulary:\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "print(vocab[:15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt0s2OAePUTT"
      },
      "source": [
        "The output from the vectorization layer is a list of integers indicating the position of the words in the vocabulary with the most frequent tokens. Let's take the first clean sample in our test set, and transform it using the `TextVectorization` layer that we have already adapted on our training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqHhG8XuPUTX",
        "outputId": "bd96a87b-98ab-4dda-fe5c-825491fe9bad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first clean textual sample in the test dataset:\n",
            "film try many thing sting political satire hollywood blockbuster sappy romantic comedy family value promo list go fail miserably enough interest keep turn end although appreciate spirit behind war inc depresses see clumsy effort especially take target reflect lack existence serious critique rather simply poor write direction production particular film critique make corporatization war poke fun way diminishes true atrocity happen reminds bit three king similarly trivializes genuine cause concern \n",
            "\n",
            "The corresponding sentiment label:  0\n"
          ]
        }
      ],
      "source": [
        "print('The first clean textual sample in the test dataset:')\n",
        "print(Xclean_test.iloc[0],'\\n')\n",
        "print('The corresponding sentiment label: ',y_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1e-FnSSPUTY",
        "outputId": "702df3f9-9dab-4cb4-c6ca-dd93e11e90bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output from the Text Vectorization: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
              "array([    3,    52,    46,    38,  5467,   962,  1776,   251,  2228,\n",
              "        3509,   609,    99,   115,   479, 11085,   558,    14,   554,\n",
              "        2907,   101,    80,   172,    92,    26,   161,   722,   824,\n",
              "         410,   173, 14939,     1,     7,  2857,   516,   176,    33,\n",
              "        1297,  2045,   228,  2009,   447,  5745,   154,   210,   231,\n",
              "          85,   334,   214,   732,     3,  5745,     5,     1,   173,\n",
              "        3240,   171,    30,  8596,   193,  3795,   195,  1279,   114,\n",
              "         177,   509,  3841,     1,  1508,   472,  1328,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0])>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "print('The output from the Text Vectorization: ')\n",
        "vectorize_layer(Xclean_test.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In the context of text vectorization (like in TensorFlow's TextVectorization layer),\n",
        "#the [UNK] token is used to represent words that are not in the vocabulary.\n",
        "#When you use TextVectorization, you define a vocabulary size limit, like:\n",
        "#The layer will keep only the top 15,000 most frequent words.\n",
        "#Any word not in the top 15,000 is considered \"unknown\"."
      ],
      "metadata": {
        "id": "pk346qdS3Qd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0wxdZDmPUTa"
      },
      "source": [
        "The zeros correspond to an empty token resulting from the padding process, and the index 1 corresponds to the UNK token. Since we are interested in training our own embeddings with W2V, we will first vectorize our train textual samples. We will then use the output to retrieve the corresponding words in string format from our vocabulary, which would serve as the input to W2V. In this way, we would generate embeddings only for the 15k most frequent tokens in our vocabulary. If we feed the cleaned textual samples to W2V without the text vectorization step, then W2V might produce embeddings also for tokens that get mapped to the UNK token during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-nNRY2WVPUTb"
      },
      "outputs": [],
      "source": [
        "X_train_vec=vectorize_layer(Xclean_train)\n",
        "X_train_words = [[vocab[w] for w in rev if vocab[w] not in ['','[UNK]']] for rev in X_train_vec]\n",
        "#we collect all train words except '' and '[UNK]', as we do not have to learn embeddings for these two tokens.\n",
        "#When we create our keras embedding layer, we will overwrite all embeddings with those learned with W2V except the embeddings\n",
        "# for the first two tokens. The embeddings of the latter will be randomly initialized, as the tokens '' and '[UNK]' do not have\n",
        "# a specific contextual meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "y0V_7SfOPUTc"
      },
      "outputs": [],
      "source": [
        "#Since this can take a while, we will save result so that we can import it later:\n",
        "with open('X_train_words.pkl','wb') as f:\n",
        "    pickle.dump(X_train_words,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ufAaI0n6PUTc"
      },
      "outputs": [],
      "source": [
        "with open('X_train_words.pkl','rb') as f:\n",
        "    X_train_words=pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeas976MPUTd"
      },
      "source": [
        "Next, we will learn the word embeddings using both CBOW and Skip-Gram. Afterward, we will create embedding layers, the continuous vectors of which will be overwritten with the embeddings from W2V:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlmtfRUfPUTd",
        "outputId": "2ed8ceda-79a8-40db-9959-d0861a41ff54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Time with CBOW in Seconds:  26.31938\n",
            "Training Time with SkipGram in Seconds:  94.57157\n"
          ]
        }
      ],
      "source": [
        "# Train CBOW and SkipGram:\n",
        "#Keep track of the training time:\n",
        "embeddings_dimension=300  #Each word will be represented by a 300-dimensional vector.\n",
        "start_time_cbow=time.time()\n",
        "w2v_model_cbow = Word2Vec(X_train_words,\n",
        "                          min_count=100, #we set the minimal token frequency to 100 to avoid creating embeddings for\n",
        "                        #rarely represented words\n",
        "                        window=10, #the size of context (context window size)\n",
        "                        epochs=50,  #(training iterations)\n",
        "                        vector_size=embeddings_dimension, #size of embedding\n",
        "                        workers=4,#for parallel computing\n",
        "                        sg  = 0   )  #means CBOW\n",
        "end_time_cbow=time.time()\n",
        "\n",
        "start_time_skipgram=time.time()\n",
        "w2v_model_skipgram = Word2Vec(X_train_words, min_count=100, window=10,\n",
        "                 epochs=50,  vector_size=embeddings_dimension, workers=4, sg  = 1)\n",
        "\n",
        "end_time_skipgram=time.time()\n",
        "print('Training Time with CBOW in Seconds: ',round(end_time_cbow-start_time_cbow,5))\n",
        "print('Training Time with SkipGram in Seconds: ',round(end_time_skipgram-start_time_skipgram,5))\n",
        "\n",
        "\n",
        "#Create corresponding Embedding Layers:\n",
        "#This creates two empty Embedding layers (one for each model), but here‚Äôs an important note:\n",
        "#‚ùóThese layers are not yet initialized with your trained Word2Vec weights.\n",
        "#They‚Äôre just initialized randomly.\n",
        "embedding_layer_cbow=keras.layers.Embedding(vocab_size,embeddings_dimension)\n",
        "embedding_layer_cbow.build(input_shape=(embeddings_dimension))\n",
        "\n",
        "embedding_layer_skipgram= keras.layers.Embedding(vocab_size,embeddings_dimension)\n",
        "embedding_layer_skipgram.build(input_shape=(embeddings_dimension))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Zz3YRTzPUTh"
      },
      "outputs": [],
      "source": [
        "#Alternatively to training your own embeddings, you can download pretrained embeddings usign the package gensim:\n",
        "#import gensim.downloader\n",
        "#w2v_model_cbow=gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CwV2ZStPUTi"
      },
      "outputs": [],
      "source": [
        "#Overwrite the weights in the embedding layers with the continuous  token representation learned with CBOW and Skip-Gram:\n",
        "\n",
        "embeddings_cbow=[]\n",
        "embeddings_skipgram=[]\n",
        "\n",
        "#Fill in the embedding matrices:\n",
        "for token_idx in range(0,len(vocab)):\n",
        "    if token_idx>1:\n",
        "        if vocab[token_idx] in w2v_model_cbow.wv:\n",
        "            embeddings_cbow.append(w2v_model_cbow.wv[vocab[token_idx]])\n",
        "        else:#take embedding corresponding to UNK token:\n",
        "            embeddings_cbow.append(embedding_layer_cbow.get_weights()[0][1])\n",
        "\n",
        "\n",
        "        if vocab[token_idx] in w2v_model_skipgram.wv:\n",
        "            embeddings_skipgram.append(w2v_model_skipgram.wv[vocab[token_idx]])\n",
        "        else:#take embedding corresponding to UNK token:\n",
        "            embeddings_skipgram.append(embedding_layer_skipgram.get_weights()[0][1])\n",
        "\n",
        "    else:#then take mebeddings of ''-token or '[UNK]'-token:\n",
        "        embeddings_cbow.append(embedding_layer_cbow.get_weights()[0][token_idx])\n",
        "        embeddings_skipgram.append(embedding_layer_skipgram.get_weights()[0][token_idx])\n",
        "\n",
        "#Overwrite the weights in the embedding layers with the corresponding W2V token embeddings.\n",
        "#You will need the embedding_layer_cbow and embedding_layer_skipgram for the 1. exercise.\n",
        "embedding_layer_cbow.set_weights(np.array([embeddings_cbow]))\n",
        "embedding_layer_skipgram.set_weights(np.array([embeddings_skipgram]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_1uwD1jPUTn"
      },
      "source": [
        "**Demo Summary**:<br>\n",
        "- we can use W2V to train our own embeddings with two models, i.e., CBOW and Skip-Gram.<br>\n",
        "- the inputs to W2V are the cleaned tokenized textual samples coming from the vocabulary built with the text vectorization layer. The latter creates the vocabulary from the set of most frequently encountered words in our cleaned dataset.\n",
        "- once we have trained our embeddings, we save each continuous vector corresponding to a token in our vocabulary in embedding matrices that we use to overwrite the weights of the embedding layers in `keras`. These embedding layers serve as a dictionary to look up words based on their indices produced from the text vectorization transformation.\n",
        "- we can use these embedding layers when implementing neural networks in `keras` to process text algorithmically. Also, we can use the output of the embedding layers as inputs to dimensionality reduction techniques that produce low dimensional vectors for plotting purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx8yakgCPUTo"
      },
      "source": [
        "## **2. Plotting Embeddings using t-SNE** (Exercise 1)<br>\n",
        "T-distributed Stochastic Neighbor Embedding (t-SNE) is a tool to generate low dimensional data, e.g., in a 2D space, from high dimensional embeddings for visualization purposes. The transformation process of t-SNE preserves the structure of the data in the low dimensional space, which enables the interpretability of the original high dimensional vectors. `Sklearn` provides you with easy access to the implementation of t-SNE (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). However, `sklearn`'s implementation is quite costly and can consume a lot of memory especially if the original data comes in a high dimensional space such as the embeddings from W2V. Therefore, in this task you will use a less computationally expensive version of t-SNE implemented in the package `openTSNE`. For this purpose, you would have to install the package with pip first. Once you have completed the installation of `openTSNE`, you are tasked with the following:<br>\n",
        "- take the first 500 samples from the cleaned `Xclean_train` dataset, and transform them with the `vectorize_layer` from the demo.\n",
        "- feed the vectorized output to the embedding layers containing the CBOW and Skip-Gram weights from the demo (`embedding_layer_cbow` and `embedding_layer_skipgram`), convert the Tensor to numpy format, and flatten the token sequences to obtain vectors of the dimensionality 300 (embedding dimension) * 100 (sequence length).\n",
        "- fit `TSNE` from `openTSNE` on the cbow and skip-gram embeddings, and store the resulting two components together with the target labels from the train set (`y_train`) in two dataframes. Name the columns of the dataframes X-Axis, Y-Axis and Sentiment Class.\n",
        "- use the package `seaborn` to plot two well-annotated scatter plots next to each other for each embedding type. Use the parameter hue from `seaborn`'s scatter plot to plot the relationship of the two-dimensional t-SNE embeddings to the sentiment class. You can check out some examples for plotting with seaborn under the following link: https://seaborn.pydata.org/generated/seaborn.scatterplot.html.  \n",
        "- provide a short interpretation of the visualization: what pattern do you observe? what could be causing this pattern? in which scenario would you observe a different pattern?   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1K1Ja28PUTp"
      },
      "outputs": [],
      "source": [
        "#!pip install openTSNE\n",
        "from openTSNE import TSNE\n",
        "num_samples=500\n",
        "#Get the embeddings using the vectorize layer and the embeddings layers from the demo:\n",
        "cbow_reviews_train=...\n",
        "#flatten the last two dimensions:\n",
        "cbow_reviews_train=...\n",
        "cbow_tsne_embeddings = TSNE().fit(...)\n",
        "\n",
        "cbow_tsne_embeddings=pd.DataFrame({...})\n",
        "\n",
        "skipgram_reviews_train=...\n",
        "#flatten the last two dimensions:\n",
        "skipgram_reviews_train=...\n",
        "skipgram_tsne_embeddings = TSNE().fit(...)\n",
        "skipgram_tsne_embeddings=pd.DataFrame({...})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEll-H5CPUTq"
      },
      "outputs": [],
      "source": [
        "fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(20,5))\n",
        "sns.scatterplot(x=...,y=...,data=cbow_tsne_embeddings,hue=...,ax=ax[0])\n",
        "ax[0].set_xlabel(...)\n",
        "ax[0].set_ylabel(...)\n",
        "ax[0].legend(mode='expand',ncols=2,loc=[0.0,-0.3],title='Sentiment Class')\n",
        "ax[0].set_title(...)\n",
        "\n",
        "sns.scatterplot(x=...,y=...,data=skipgram_tsne_embeddings,hue=...,ax=ax[1])\n",
        "ax[1].set_xlabel(...)\n",
        "ax[1].set_ylabel(...)\n",
        "ax[1].set_title(...)\n",
        "\n",
        "plt.show()\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyoXJmocPUTs"
      },
      "source": [
        "**Interpretation**:<br>\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtcR4qXYPUTs"
      },
      "source": [
        "## **3. MLP-based Neural Network with W2V Embeddings for Sentiment Classification** (Exercise 2)<br>\n",
        "In this exercise, you are tasked with the implementation of two MLP-based neural networks for sentiment classification on the IMDB dataset using the W2V embeddings that we generated with CBOW and Skip-Gram in the demo part of this notebook:<br>\n",
        "- build two sequential keras models (cbow_mlp_model and skipgram_mlp_model), which consists of six layers: <br>\n",
        "an input layer with shape = (1) and  dtype='string', the `vectorize_layer` from the demo, an embedding layer initialized with the vocabulary size = 15k, the embedding dimension = 300 and the input sequence length = 100, a GlobalAveragePooling1D layer, a GELU-based hidden layer with 10 units and a final Sigmoid-based prediction layer.<br>\n",
        "\n",
        "- after compiling the models with the Adam optimizer and the binary crossentropy loss, overwrite the weights in the embedding layers with the embedding matrices from the demo `embeddings_cbow` and `embeddings_skipgram` by using the function `set_weights()`.\n",
        "- make two runs with each model for 10 epochs with a batch size of 32: in the first run freeze the W2V weights, in the second run set the weights in the embedding layers to trainable. You should use the cleaned version of the train and test datasets from the demo when fitting the models, i.e, `Xclean_train` and `Xclean_test`.\n",
        "\n",
        "- report the results in terms of AUC score, precision and recall score in a tabular overview. You can use the threshold of 0.5, so that you convert the keras probabilities into binary classes for the computation of the precision and recall scores.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VjGDSqePUTy"
      },
      "outputs": [],
      "source": [
        "predictions_results_frame=[]\n",
        "\n",
        "for nr_run in [0,1]:\n",
        "  #Create the models: based on the run (0 or 1) set the trainable parameter of the Embedding layer to True or False\n",
        "  cbow_mlp_model = Sequential([...])\n",
        "\n",
        "  skipgram_mlp_model = Sequential([...])\n",
        "\n",
        "  #Compile the models and overwrite the embedding layer weights with the embeddings from W2V:\n",
        "  cbow_mlp_model.compile(...)\n",
        "  cbow_mlp_model.layers[1].set_weights(np.array([embeddings_cbow]))\n",
        "\n",
        "  skipgram_mlp_model.compile(...)\n",
        "  skipgram_mlp_model.layers[1].set_weights(np.array([embeddings_skipgram]))\n",
        "\n",
        "  #Fit the cbow-based model and make predictions:\n",
        "  cbow_mlp_model.fit(Xclean_train.to_numpy().reshape(-1,1),\n",
        "                     y_train,\n",
        "                     ...)\n",
        "\n",
        "  cbow_proba_pr=cbow_mlp_model.predict(Xclean_test.to_numpy().reshape(-1,1),verbose=0).flatten()\n",
        "  cbow_class_pr=np.where(...)\n",
        "  predictions_results_frame.append([...])\n",
        "\n",
        "  #Do the same for the skipgram-based MLP:\n",
        "  skipgram_mlp_model.fit(Xclean_train.to_numpy().reshape(-1,1),\n",
        "                         y_train,\n",
        "                         ...)\n",
        "\n",
        "  skipgram_proba_pr=skipgram_mlp_model.predict(Xclean_test.to_numpy().reshape(-1,1),verbose=0).flatten()\n",
        "  skipgram_class_pr=np.where(...)\n",
        "  predictions_results_frame.append([...])\n",
        "\n",
        "#Put the results in a dataframe:\n",
        "results_overview=pd.DataFrame(np.around(np.array(predictions_results_frame),3),columns=['Recall Score','Precision Score','ROC Auc Score'],\n",
        "            index=[...])#name the indices in such way that it is clear which run was using trainable vs. non-trainable embedding layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HMH4BBpPUTz"
      },
      "outputs": [],
      "source": [
        "results_overview.sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQYKOUT0PUTz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}